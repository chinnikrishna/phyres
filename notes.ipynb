{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physical Reasoning\n",
    "Physical reasoning refers to ability of humans to reason about physical interactions between objects qualitatively. Building artificial agents which share a similar intuition allows them to adapt to new environment sample efficiently. It also allows them to operate in uncertain environments. Facebook introduced a benchmark called PHYRE to explore this ability, https://phyre.ai/. A similar but more challenging benchmark called The Virtual Tools is introduced by MIT, https://sites.google.com/view/virtualtoolsgame/home.\n",
    "\n",
    "<br>\n",
    "\n",
    "### PHYRE\n",
    "PHYRE benchmark consists of a physics simulator which models gravity and perfect collisions. The central theme of all the tasks is to place one or two balls in the environment such that by the end of the simulation green object touches blue object, which is the goal state. \n",
    "\n",
    "(https://player.phyre.ai/#/task/00000:000)\n",
    "\n",
    "(https://player.phyre.ai/#/task/00006:004)\n",
    "\n",
    "![Tier B tasks](./media/tasks.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Details\n",
    "1. Each tier consists of 25 templates and each template consists of 100 tasks. So across two tiers we have 5000 tasks to learn from.\n",
    "2. An initial scene of 256 x 256 is provided to the agent for providing context. Each object in the scene is coded by a number which is as follows\n",
    "    * 0 - Background\n",
    "    * 1 - Object by agent\n",
    "    * 2 - Dynamic goal object\n",
    "    * 3 - Dynamic goal subject\n",
    "    * 4 - Static goal subject\n",
    "    * 5 - Dynamic confounding body\n",
    "    * 6 - Static confounding body\n",
    "\n",
    "3. An action is performed by sending a tuple (x, y, r) to the simulator. (x, y) represents the position while r represents the radius of ball placed. All these values are between 0 and  1. If an action is outside the scene frame or on any object it results in an invalid action.\n",
    "4. Simulator returns back with the following reward\n",
    "    * 1  - Solved the task\n",
    "    * 0  - Invalid action\n",
    "    * -1  - Did not solve the task\n",
    "5. Efficiency of the agent is measured by number of actions that are attempted to solve the task during testing phase. Fewer attempts correspond to greater efficiency. A new metric AUCCESS is coined for emphasis on fewer attempts. AUCCESS is defined as follows\n",
    "\n",
    "$ AUCCESS =  \\frac {\\sum_k w_k . s_k} {\\sum_k w_k}$\n",
    "\n",
    "$ w_k = log(k + 1) - log(k)  \\quad k \\in \\{1,...100\\}$\n",
    "\n",
    "$ s_k $ is success percentage at k attempts\n",
    "\n",
    "\n",
    "### Experiments in the paper\n",
    "The following agents were developed in paper to establish baselines\n",
    "\n",
    "1. RAND - Random agent which uniformly samples valid actions\n",
    "2. MEM - Remembers which random action worked during training and uses them during inference\n",
    "3. MEM-O - Continues to train during testing phase also\n",
    "4. DQN - Deep Q-Network which trained by minimizing the cross-entropy between the soft prediction and the observed reward. There are so many finer details here.\n",
    "5. DQN-O - Similar to DQN but continues to learn during testing phase too.\n",
    "\n",
    "![Results](./media/paper_stoa_res.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
